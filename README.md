# Medical Insurance Cost Prediction

## Project Overview

This project aims to predict individual medical insurance costs based on a set of personal attributes. The analysis is divided into two main phases:

1.  **Exploratory Data Analysis (EDA):** A deep dive into the dataset to understand its structure, identify key patterns, and uncover relationships between features and medical expenses.
2.  **Model Building:** Development and evaluation of several regression models to accurately predict the costs.

The final outcome is a comparison of four different regression models, with the Gradient Boosting Regressor emerging as the most effective predictor.

---

## Dataset

The project uses the `insurance.csv` dataset, a popular dataset for regression tasks. It contains the following columns:

*   **age**: Age of the primary beneficiary.
*   **sex**: Insurance contractor gender (female, male).
*   **bmi**: Body mass index.
*   **children**: Number of children covered by health insurance.
*   **smoker**: Smoking status (yes, no).
*   **region**: The beneficiary's residential area in the US (northeast, southeast, southwest, northwest).
*   **expenses**: Individual medical costs billed by health insurance (the target variable).

---

## Project Structure

This repository contains the following key files:

*   `EDA 1 (3).ipynb`: The Jupyter Notebook for the Exploratory Data Analysis. This notebook performs data cleaning, visualization, and feature engineering. It generates `insurance_processed.csv`.
*   `Model_Building.ipynb`: The Jupyter Notebook for building and evaluating the regression models. It uses the processed data to train and compare four different models.
*   `insurance.csv`: The raw, original dataset.
*   `insurance_processed.csv`: The processed dataset generated by the EDA notebook, which includes the engineered features.
*   `README.md`: This file, providing a summary of the project.

---

## Exploratory Data Analysis (EDA) - Key Findings

The EDA was crucial for understanding the data and guiding our modeling strategy. The main insights were:

1.  **Right-Skewed Target**: The `expenses` variable is heavily right-skewed, with most individuals having low costs and a few having extremely high costs. This suggested that a log transformation would be beneficial for the linear models.
2.  **Smoking is the Primary Driver**: The most significant factor influencing medical costs is smoking status. Smokers, on average, have drastically higher expenses than non-smokers.
3.  **The BMI-Smoker Interaction**: The most powerful insight was the interaction effect between BMI and smoking. For non-smokers, BMI has a very weak correlation with expenses. However, for smokers, costs skyrocket for those with a BMI over 30 (obese).
4.  **Feature Engineering**: Based on the interaction insight, two new features were created:
    *   `bmi_category`: Categorizes individuals as Underweight, Normal, Overweight, or Obese.
    *   `obese_smoker`: A binary flag (1 or 0) that identifies individuals in the high-risk group of obese smokers. This feature proved to be highly predictive.

![BMI vs Expenses Scatter Plot](path/to/your/bmi_scatter_plot.png)
*(You should save the BMI vs. Expenses scatter plot from your notebook as an image and add it here)*

---

## Model Building and Evaluation

### 1. Preprocessing

*   **Log Transformation**: The target variable `expenses` was transformed using `np.log1p` to normalize its distribution.
*   **One-Hot Encoding**: Categorical features (`sex`, `smoker`, `region`) were converted to numerical format.
*   **Data Splitting**: The data was split into an 80% training set and a 20% testing set.

### 2. Models Trained

Four different regression models were trained and evaluated:
1.  Multiple Linear Regression (Baseline)
2.  Ridge Regression (Regularized Linear Model)
3.  Random Forest Regressor (Ensemble Model)
4.  Gradient Boosting Regressor (Ensemble Model)

### 3. Performance Results

The models were evaluated based on their R-squared (R²) score and Mean Absolute Error (MAE) on the test set. The Gradient Boosting model performed the best, closely followed by the Random Forest.

![Model Performance Comparison](path/to/your/model_comparison_chart.png)
*(You should save the final model comparison bar chart from your notebook as an image and add it here)*

| Model                       | R-squared (R²) | Mean Absolute Error (MAE) |
| --------------------------- | -------------- | ------------------------- |
| Gradient Boosting Regressor | 0.8676         | $2,045.68                 |
| Random Forest Regressor     | 0.8472         | $2,092.97                 |
| Multiple Linear Regression  | 0.8215         | $3,698.86                 |
| Ridge Regression            | 0.8215         | $3,674.08                 |

---

## Conclusion

The ensemble models **(Gradient Boosting and Random Forest)** significantly outperformed the linear models. This is because they are better at capturing the complex, non-linear relationships and interaction effects that were discovered during the EDA.

The **Gradient Boosting Regressor** emerged as the top-performing model, explaining nearly **87%** of the variance in medical expenses and achieving the lowest average prediction error of approximately **$2,046**.

This project highlights the importance of a thorough EDA phase, as the insights gained directly informed the feature engineering and modeling choices that led to a high-performance predictive model.

---

## How to Run This Project

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/your-repo-name.git
    cd your-repo-name
    ```

2.  **Install the required libraries:**
    ```bash
    pip install pandas numpy scikit-learn matplotlib seaborn jupyter
    ```

3.  **Run the notebooks:**
    *   First, run the `EDA 1 (3).ipynb` notebook from start to finish. This will perform the analysis and create the `insurance_processed.csv` file.
    *   Next, run the `Model_Building.ipynb` notebook to preprocess the data, train the models, and see the final evaluation.
